\relax 
\citation{ref13}
\citation{ref7}
\citation{ref35}
\citation{ref2}
\citation{ref5}
\citation{ref38}
\citation{ref24}
\citation{ref15}
\citation{ref21}
\citation{ref32}
\citation{ref2}
\citation{ref19}
\citation{ref27}
\citation{ref16}
\citation{ref18}
\citation{ref9}
\citation{ref12}
\citation{ref4}
\citation{ref27}
\citation{ref28}
\citation{ref22}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{3}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Background}{3}{}\protected@file@percent }
\citation{ref34}
\citation{ref17}
\citation{ref18}
\citation{ref9}
\citation{ref5}
\citation{ref2}
\citation{ref35}
\citation{ref10}
\citation{ref11}
\citation{ref1}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The Transformer - model architecture.}}{4}{}\protected@file@percent }
\newlabel{fig:arch}{{1}{4}{}{figure.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Model Architecture}{4}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Encoder and Decoder Stacks}{4}{}\protected@file@percent }
\citation{ref2}
\citation{ref3}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel.}}{5}{}\protected@file@percent }
\newlabel{fig:attention}{{2}{5}{}{figure.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Attention}{5}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Scaled Dot-Product Attention}{5}{}\protected@file@percent }
\citation{ref38}
\citation{ref2}
\citation{ref9}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Multi-Head Attention}{6}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.3}Applications of Attention in our Model}{6}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Position-wise Feed-Forward Networks}{6}{}\protected@file@percent }
\citation{ref30}
\citation{ref9}
\citation{ref9}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Embeddings and Softmax}{7}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Positional Encoding}{7}{}\protected@file@percent }
\citation{ref12}
\citation{ref38}
\citation{ref31}
\citation{ref18}
\citation{ref6}
\@writefile{toc}{\contentsline {section}{\numberline {4}Why Self-Attention}{8}{}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types. $n$ is the sequence length, $d$ is the representation dimension, $k$ is the kernel size of convolutions and $r$ the size of the neighborhood in restricted self-attention.}}{8}{}\protected@file@percent }
\newlabel{tab:comparison}{{1}{8}{}{table.1}{}}
\citation{ref3}
\citation{ref38}
\citation{ref20}
\@writefile{toc}{\contentsline {section}{\numberline {5}Training}{9}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Training Data and Batching}{9}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Hardware and Schedule}{9}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Optimizer}{9}{}\protected@file@percent }
\citation{ref33}
\citation{ref36}
\citation{ref38}
\citation{ref38}
\citation{ref18}
\citation{ref39}
\citation{ref38}
\citation{ref9}
\citation{ref32}
\citation{ref39}
\citation{ref38}
\citation{ref9}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Regularization}{10}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Results}{10}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Machine Translation}{10}{}\protected@file@percent }
\citation{ref9}
\citation{ref37}
\citation{ref25}
\citation{ref37}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces The Transformer achieves better BLEU scores than previous state-of-the-art models on the English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.}}{11}{}\protected@file@percent }
\newlabel{tab:results}{{2}{11}{}{table.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Model Variations}{11}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}English Constituency Parsing}{11}{}\protected@file@percent }
\citation{ref8}
\citation{ref37}
\citation{ref29}
\citation{ref37}
\citation{ref29}
\citation{ref40}
\citation{ref8}
\citation{ref40}
\citation{ref14}
\citation{ref26}
\citation{ref37}
\citation{ref23}
\citation{ref8}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Variations on the Transformer architecture. Unlisted values are identical to those of the base model. All metrics are on the English-to-German translation development set, newstest2013. Listed perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to per-word perplexities.}}{12}{}\protected@file@percent }
\newlabel{tab:variations}{{3}{12}{}{table.3}{}}
\bibcite{ref1}{1}
\bibcite{ref2}{2}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces The Transformer generalizes well to English constituency parsing (Results are on Section 23 of WSJ)}}{13}{}\protected@file@percent }
\newlabel{tab:parsing}{{4}{13}{}{table.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusion}{13}{}\protected@file@percent }
\bibcite{ref3}{3}
\bibcite{ref4}{4}
\bibcite{ref5}{5}
\bibcite{ref6}{6}
\bibcite{ref7}{7}
\bibcite{ref8}{8}
\bibcite{ref9}{9}
\bibcite{ref10}{10}
\bibcite{ref11}{11}
\bibcite{ref12}{12}
\bibcite{ref13}{13}
\bibcite{ref14}{14}
\bibcite{ref15}{15}
\bibcite{ref16}{16}
\bibcite{ref17}{17}
\bibcite{ref18}{18}
\bibcite{ref19}{19}
\bibcite{ref20}{20}
\bibcite{ref21}{21}
\bibcite{ref22}{22}
\bibcite{ref23}{23}
\bibcite{ref24}{24}
\bibcite{ref25}{25}
\bibcite{ref26}{26}
\bibcite{ref27}{27}
\bibcite{ref28}{28}
\bibcite{ref29}{29}
\bibcite{ref30}{30}
\bibcite{ref31}{31}
\bibcite{ref32}{32}
\bibcite{ref33}{33}
\bibcite{ref34}{34}
\bibcite{ref35}{35}
\bibcite{ref36}{36}
\bibcite{ref37}{37}
\bibcite{ref38}{38}
\bibcite{ref39}{39}
\bibcite{ref40}{40}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of the verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for the word ‘making’. Different colors represent different heads. Best viewed in color.}}{18}{}\protected@file@percent }
\newlabel{fig:viz1}{{3}{18}{}{figure.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top: Full attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5 and 6. Note that the attentions are very sharp for this word.}}{18}{}\protected@file@percent }
\newlabel{fig:viz2}{{4}{18}{}{figure.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Many of the attention heads exhibit behaviour that seems related to the structure of the sentence. We give two such examples above, from two different heads from the encoder self-attention at layer 5 of 6. The heads clearly learned to perform different tasks.}}{18}{}\protected@file@percent }
\newlabel{fig:viz3}{{5}{18}{}{figure.5}{}}
\newlabel{LastPage}{{7}{18}{}{figure.5}{}}
\gdef\lastpage@lastpage{18}
\gdef\lastpage@lastpageHy{}
\gdef \@abspage@last{18}
